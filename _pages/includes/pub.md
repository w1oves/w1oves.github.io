
# üìù Publications 
*(\* denotes equal contribution.)*
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arXiV</div><img src='images/youtu-vl-overview.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

### First Author
[Youtu-VL: Unleashing Visual Potential via Unified Vision-Language Supervision](https://arxiv.org/abs/2601.19798)

[**Zhixiang Wei**](https://zxwei.site), Youtu-VL Team.

[[ü§óHuggingface](https://huggingface.co/tencent/Youtu-VL-4B-Instruct)] [![](https://img.shields.io/github/stars/TencentCloudADP/youtu-vl?style=social&label=youtu-vl+Stars)](https://github.com/TencentCloudADP/youtu-vl)

- Youtu-VL is a lightweight yet robust Vision-Language Model (VLM) built on the Youtu-LLM with 4B parameters. It pioneers Vision-Language Unified Autoregressive Supervision (VLUAS), which markedly strengthens visual perception and multimodal understanding. This enables a standard VLM to perform vision-centric tasks without task-specific additions. Across benchmarks, Youtu-VL stands out for its versatility, achieving competitive results on both vision-centric and general multimodal tasks.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2025</div><img src='images/iccv25.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

### First Author
[HQCLIP: Leveraging Vision-Language Models to Create High-Quality Image-Text Datasets and CLIP Models](https://arxiv.org/abs/2507.22431)

[**Zhixiang Wei**](https://zxwei.site)\*, [Guangting Wang](https://scholar.google.com/citations?user=cKY8e8sAAAAJ&hl=zh-CN)\*, [Xiaoxiao Ma](https://krennic999.github.io/), et al.

[[Project page](https://zxwei.site/hqclip/)] [![](https://img.shields.io/github/stars/w1oves/hqclip?style=social&label=hqclip+Stars)](https://github.com/w1oves/hqclip)

- We generated detailed, bidirectional long-text descriptions for **1.3 billion** images and pretrained/fine-tuned CLIP based on this dataset. Building upon this foundation, we propose a novel CLIP training framework that combines both bidirectional supervision and label classification losses. This framework achieves SoTA results on zero-shot classification, retrieval, and other tasks at the same data scale.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2024</div><img src='images/rein.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

### First Author
[Stronger, Fewer, & Superior: Harnessing Vision Foundation Models for Domain Generalized Semantic Segmentation](https://arxiv.org/pdf/2312.04265.pdf) \\
[**Zhixiang Wei**](https://zxwei.site)\*, [Lin Chen](https://lin-chen.site/)\*, Yi Jin\*, [Xiaoxiao Ma](https://krennic999.github.io/), et al.

[[Project page](https://zxwei.site/rein/)]  [![](https://img.shields.io/github/stars/w1oves/Rein?style=social&label=Rein+Stars)](https://github.com/w1oves/Rein)  <strong><span class='show_paper_citations' data='4FA6C0AAAAAJ:qjMakFHDy7sC'></span></strong>

- We propose the Reins framework, which efficiently fine-tunes vision foundation models for the domain generalized semantic segmentation (DGSS) task with just 1% trainable parameters, surprisingly surpassing full parameter fine-tuning. And Reins builds a new SOTA in various DGSS benchmarks.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2023</div><img src='images/dtp.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

### First Author
[Disentangle then Parse: Night-time Semantic Segmentation with Illumination Disentanglement](https://arxiv.org/pdf/2307.09362.pdf)<span class='show_paper_citations' data='i5W4i9YAAAAJ:UeHWp8X0CEIC'></span> \\
[**Zhixiang Wei**](https://zxwei.site)\*, [Lin Chen](https://lin-chen.site/)\*, et al.

[![](https://img.shields.io/github/stars/w1oves/DTP?style=social&label=DTP+Stars)](https://github.com/w1oves/DTP)
  - We propose a novel nigh-time semantic segmentation paradigm, i.e., disentangle then parse (DTP), which explicitly disentangles night-time images into light-invariant reflectance and light-specific illumination components and then recognizes semantics based on their adaptive fusion.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TPAMI 2025 <img src='images/crossearth.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

### Co-First Author
[CrossEarth: Geospatial Vision Foundation Model for Domain Generalizable Remote Sensing Semantic Segmentation](https://arxiv.org/pdf/2410.22629)<span class='show_paper_citations' data='i5W4i9YAAAAJ:2osOgNQ5qMEC'></span> \\
[Ziyang Gong](https://github.com/Cuzyoung)\*, [**Zhixiang Wei**](https://zxwei.site)\*, et al.

https://github.com/VisionXLab/CrossEarth
[![](https://img.shields.io/github/stars/VisionXLab/CrossEarth?style=social&label=CrossEarth+Stars)](https://github.com/VisionXLab/CrossEarth)
- we introduce the first vision foundation model for Remote Sensing Domain Generalizatio semantic segmentation, CrossEarth.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2022 <span style="color:yellow">(Spotlight)</span></div><img src='images/ddb.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

### Co-First Author
[Deliberated Domain Bridging for Domain Adaptive Semantic Segmentation](https://arxiv.org/pdf/2209.07695.pdf)<span class='show_paper_citations' data='i5W4i9YAAAAJ:2osOgNQ5qMEC'></span> \\
[Lin Chen](https://lin-chen.site/)\*, [**Zhixiang Wei**](https://zxwei.site)\*, [Xin Jin](https://www.eitech.edu.cn/?tid=40&p=teacher)\*, et al.

[![](https://img.shields.io/github/stars/xiaoachen98/DDB?style=social&label=DDB+Stars)](https://github.com/xiaoachen98/DDB)
- We leverage the complementary characteristics of the coarse-wise and fine-wise data mixing techniques to progressively transfer the knowledge from the source to the target domain.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2024</div><img src='images/MPI.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

### Co-First Author
[Masked Pre-trained Model Enables Universal Zero-shot Denoiser](https://arxiv.org/abs/2401.14966) \\
, [Xiaoxiao Ma](https://krennic999.github.io/)\*, [**Zhixiang Wei**](https://zxwei.site)\*, et al.  

[![](https://img.shields.io/github/stars/walker-hyf/ECSS?style=social&label=Code+Stars)](https://github.com/krennic999/MPI)
- MPI is a zero-shot denoising pipeline designed for many types of noise degradations.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2022</div><img src='images/daln.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

### Co-Author
[Reusing the Task-specific Classifier as a Discriminator: Discriminator-free Adversarial Domain Adaptation](https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_Reusing_the_Task-Specific_Classifier_as_a_Discriminator_Discriminator-Free_Adversarial_Domain_CVPR_2022_paper.pdf)<span class='show_paper_citations' data='i5W4i9YAAAAJ:9yKSN-GCB0IC'></span> \\
[Lin Chen](https://lin-chen.site/), [**Zhixiang Wei**](https://zxwei.site), [Xin Jin](https://www.eitech.edu.cn/?tid=40&p=teacher), [Enhong Chen](http://staff.ustc.edu.cn/~cheneh/).

[![](https://img.shields.io/github/stars/xiaoachen98/DALN?style=social&label=DALN+Stars)](https://github.com/xiaoachen98/DALN)
- We reuse the category classifier as a discriminator to form a discriminator-free adversarial learning framework.
</div>
</div>

``NeurIPS 2025`` [Towards Better & Faster Autoregressive Image Generation: From the Perspective of Entropy](), [Xiaoxiao Ma](https://krennic999.github.io/), [**Zhixiang Wei**](https://zxwei.site).


