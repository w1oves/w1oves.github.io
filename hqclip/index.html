<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>[ICCV 2025] HQ-CLIP: Enhancing CLIP with Large Vision-Language Models</title>
  
  <!-- Meta Tags -->
  <meta name="description" content="[ICCV 2025] HQ-CLIP leverages LVLMs to create high-quality image-text datasets and enhance CLIP models through multi-grained supervision">
  <meta name="keywords" content="HQ-CLIP, CLIP, Vision-Language Models, Multi-modal Learning, Dataset Refinement">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Social Sharing -->
  <meta property="og:title" content="HQ-CLIP: Creating High-Quality Image-Text Datasets with LVLMs"/>
  <meta property="og:image" content="https://zxwei.site/images/iccv25.png"/>
  <meta property="og:url" content="https://zxwei.site/hqclip"/>
  
  <!-- Fonts -->
  <link href="https://fonts.googleapis.com/css2?family=Libre+Baskerville:wght@400;700&family=Roboto:wght@300;400;500;700&display=swap" rel="stylesheet">

  <!-- Styles -->
  <style>
    :root {
      --primary: #2c5e92;
      --secondary: #4a7bab;
      --accent: #e63946;
      --light: #f8f9fa;
      --dark: #212529;
      --border: #dee2e6;
      --shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -2px rgba(0, 0, 0, 0.1);
    }
    
    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }
    
    body {
      font-family: 'Roboto', sans-serif;
      color: #333;
      line-height: 1.6;
      background-color: #fff;
      overflow-x: hidden;
    }
    
    /* Header Section */
    .hero {
      background: linear-gradient(135deg, var(--primary) 0%, var(--secondary) 100%);
      color: white;
      padding: 3rem 1rem;
      position: relative;
      /* KEY FIX: Added z-index to lift this section above subsequent sections */
      z-index: 3; 
    }
    
    .hero::before {
      content: "";
      position: absolute;
      top: 0;
      left: 0;
      right: 0;
      bottom: 0;
      background: url('data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxMDAlIiBoZWlnaHQ9IjEwMCUiPjxkZWZzPjxwYXR0ZXJuIGlkPSJwYXR0ZXJuIiB3aWR0aD0iNDAiIGhlaWdodD0iNDAiIHBhdHRlcm5Vbml0cz0idXNlclNwYWNlT25Vc2UiIHBhdHRlcm5UcmFuc2Zvcm09InJvdGF0ZSg0NSkiPjxyZWN0IHg9IjAiIHk9IjAiIHdpZHRoPSIyMCIgaGVpZ2h0PSIyMCIgZmlsbD0icmdiYSgyNTUsIDI1NSwgMjU1LCAwLjA1KSIgLz48L3BhdHRlcm4+PC9kZWZzPjxyZWN0IHdpZHRoPSIxMDAlIiBoZWlnaHQ9IjEwMCUiIGZpbGw9InVybCgjcGF0dGVybikiLz48L3N2Zz4=');
      opacity: 0.15;
    }
    
    .container {
      max-width: 1200px;
      margin: 0 auto;
      padding: 0 1.5rem;
      position: relative;
      z-index: 2;
    }
    
    .hero-body {
      padding: 2rem 0;
      text-align: center;
    }
    
    .publication-title {
      font-family: 'Libre Baskerville', serif;
      font-size: 2.8rem;
      font-weight: 700;
      margin-bottom: 1rem;
      text-shadow: 0 2px 4px rgba(0,0,0,0.2);
      line-height: 1.2;
    }
    
    .hero-subtitle {
      font-size: 1.5rem;
      font-weight: 300;
      margin-bottom: 2rem;
      max-width: 800px;
      margin-left: auto;
      margin-right: auto;
    }
    
    .authors {
      font-size: 1.1rem;
      margin-bottom: 1.5rem;
      text-shadow: 0 1px 2px rgba(0,0,0,0.2);
    }
    
    .author-block {
      display: inline-block;
      margin: 0 8px 5px 0;
      padding: 0 8px;
    }
    
    .affiliations {
      font-size: 0.95rem;
      margin-bottom: 1.5rem;
      max-width: 800px;
      margin-left: auto;
      margin-right: auto;
    }
    
    .divider {
      height: 1px;
      background: rgba(255,255,255,0.25);
      margin: 1.5rem auto;
      max-width: 600px;
    }
    
    .publication-links {
      display: flex;
      justify-content: center;
      flex-wrap: wrap;
      gap: 15px;
      margin-top: 2rem;
    }
    
    .btn {
      display: inline-flex;
      align-items: center;
      justify-content: center;
      gap: 10px;
      padding: 12px 24px;
      background: rgba(255,255,255,0.95);
      color: var(--primary);
      border-radius: 8px;
      border: none;
      text-decoration: none;
      font-weight: 500;
      font-size: 1rem;
      font-family: 'Roboto', sans-serif;
      transition: all 0.3s ease;
      box-shadow: var(--shadow);
      text-align: center;
      min-width: 160px;
      cursor: pointer;
    }
    
    .btn:hover {
      transform: translateY(-3px);
      box-shadow: 0 6px 12px -2px rgba(0,0,0,0.15), 0 3px 7px -3px rgba(0,0,0,0.15);
      background: white;
      color: var(--accent);
    }
    
    .btn-primary {
      background: white;
      color: var(--accent);
      font-weight: 600;
    }
    
    .btn svg {
      width: 1.2em;
      height: 1.2em;
    }
    
    section {
      padding: 4rem 0;
      position: relative;
    }
    
    .section-header {
      text-align: center;
      margin-bottom: 3rem;
      position: relative;
      padding-bottom: 1rem;
    }
    
    .section-header h2 {
      font-family: 'Libre Baskerville', serif;
      font-size: 2.2rem;
      color: var(--primary);
      position: relative;
      display: inline-block;
      padding: 0 20px;
    }
    
    .section-header h2::after {
      content: "";
      position: absolute;
      left: 50%;
      transform: translateX(-50%);
      bottom: -10px;
      width: 60%;
      height: 3px;
      background: var(--accent);
      border-radius: 2px;
    }
    
    .content {
      font-size: 1.05rem;
      line-height: 1.8;
      color: #444;
      max-width: 800px;
      margin: 0 auto;
    }
    
    .content p {
      margin-bottom: 1.5rem;
    }
    
    .highlight {
      background: linear-gradient(120deg, rgba(230,57,70,0.1), rgba(230,57,70,0));
      padding: 3px 6px;
      border-radius: 4px;
      font-weight: 500;
      color: var(--accent);
    }
    
    .image-caption {
      text-align: center;
      font-size: 0.9rem;
      color: #666;
      margin-top: 1rem;
      font-style: italic;
    }
    
    .full-width-image-container {
        max-width: 1000px;
        margin: 2rem auto 0 auto;
        padding: 1rem;
        background: white;
        border-radius: 10px;
        box-shadow: var(--shadow);
    }

    .full-width-image-container img {
        width: 100%;
        height: auto;
        display: block;
        border-radius: 6px;
    }

    .problem-solution {
      background: var(--light);
    }
    
    .ps-container {
      display: flex;
      flex-wrap: wrap;
      gap: 3rem;
      margin-top: 2rem;
    }
    
    .ps-item {
      flex: 1;
      min-width: 300px;
      padding: 2rem;
      background: white;
      border-radius: 10px;
      box-shadow: var(--shadow);
      transition: transform 0.3s ease;
      border-top: 4px solid var(--secondary);
    }
    
    .ps-item:hover {
      transform: translateY(-10px);
    }
    
    .ps-title {
      font-size: 1.4rem;
      color: var(--primary);
      margin-bottom: 1.2rem;
      font-weight: 600;
      position: relative;
      padding-left: 20px;
    }
    
    .ps-title::before {
      content: "";
      position: absolute;
      left: 0;
      top: 50%;
      transform: translateY(-50%);
      height: 25px;
      width: 4px;
      background: var(--accent);
      border-radius: 2px;
    }
    
    .methodology-items {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
      gap: 2rem;
      margin-top: 3rem;
    }
    
    .method-item {
      background: white;
      border-radius: 10px;
      padding: 2rem;
      box-shadow: var(--shadow);
      border-top: 4px solid var(--accent);
      transition: transform 0.3s ease, box-shadow 0.3s ease;
    }
    
    .method-item:hover {
      transform: translateY(-5px);
      box-shadow: 0 10px 20px -5px rgba(0,0,0,0.15);
    }
    
    .method-icon {
      font-size: 2.5rem;
      color: var(--accent);
      margin-bottom: 1rem;
      display: inline-block;
    }
    
    .method-title {
      font-size: 1.3rem;
      color: var(--primary);
      margin-bottom: 1rem;
      font-weight: 600;
    }
    
    .results-section {
      background: var(--light);
    }
    
    .result-block {
        margin-bottom: 4rem;
    }

    .result-block:last-child {
        margin-bottom: 0;
    }

    .result-title {
      font-size: 1.5rem;
      font-weight: 600;
      color: var(--primary);
      margin-bottom: 0.5rem;
      text-align: center;
    }

    .result-description {
        text-align: center;
        color: #555;
        max-width: 700px;
        margin: 0 auto 1.5rem auto;
    }
    
    .contributions {
      background: linear-gradient(to right, var(--primary), var(--secondary));
      color: white;
    }
    
    .contributions .section-header h2, .contributions .content {
      color: white;
    }
    
    .contrib-list {
      list-style-type: none;
      max-width: 800px;
      margin: 0 auto;
    }
    
    .contrib-list li {
      margin-bottom: 1.5rem;
      padding-left: 2.5rem;
      position: relative;
      font-size: 1.1rem;
    }
    
    .contrib-list li::before {
      content: "‚úì";
      position: absolute;
      left: 0;
      top: 0;
      color: var(--accent);
      font-weight: bold;
      font-size: 1.5rem;
      line-height: 1;
    }
    
    .bibtex-section {
      background: var(--dark);
      color: white;
    }
    
    .bibtex-section .section-header h2 {
      color: white;
    }
    
    .bibtex-code {
      background: #2c3e50;
      padding: 2rem;
      border-radius: 8px;
      font-family: monospace;
      white-space: pre-wrap;
      overflow-x: auto;
      margin-top: 2rem;
      box-shadow: 0 10px 20px rgba(0,0,0,0.2);
      border-left: 4px solid var(--accent);
      text-align: left;
    }
    
    footer {
      background: var(--dark);
      color: rgba(255,255,255,0.7);
      padding: 3rem 0;
      text-align: center;
    }
    
    .footer-text {
      max-width: 700px;
      margin: 0 auto;
      font-size: 0.95rem;
      line-height: 1.6;
    }
    
    .copyright {
      margin-top: 1.5rem;
      font-size: 0.85rem;
      color: rgba(255,255,255,0.5);
    }
    
    /* --- DROPDOWN MENU REWORK --- */
    .dropdown {
      position: relative;
      display: inline-block;
    }
    
    .dropdown-btn::after {
      content: "‚ñº";
      margin-left: 8px;
      font-size: 0.7em;
      transition: transform 0.3s ease;
    }
    
    /* Rotate arrow when dropdown is active */
    .dropdown-btn.active::after {
      transform: rotate(180deg);
    }
    
    .dropdown-content {
      display: none; /* Hidden by default */
      position: absolute;
      background-color: white;
      min-width: 100%;
      box-shadow: 0 8px 16px rgba(0,0,0,0.2);
      z-index: 1000;
      border-radius: 6px;
      overflow: hidden;
      margin-top: 12px;
      left: 0;
      animation: fadeIn 0.3s ease;
      text-align: left;
    }
    
    /* New class to be toggled by JavaScript */
    .dropdown-content.show {
      display: block;
    }
    
    .dropdown-content::before {
      content: '';
      position: absolute;
      top: -6px;
      left: 50%;
      transform: translateX(-50%);
      border-width: 0 6px 6px 6px;
      border-style: solid;
      border-color: transparent transparent white transparent;
    }
    
    .dropdown-content a {
      color: var(--primary);
      padding: 12px 16px;
      text-decoration: none;
      display: block;
      font-size: 0.95rem;
      transition: background-color 0.2s, color 0.2s;
      border-bottom: 1px solid #f0f0f0;
    }
    
    .dropdown-content a:last-child {
      border-bottom: none;
    }
    
    .dropdown-content a:hover {
      background-color: var(--light);
      color: var(--accent);
    }
    
    @keyframes fadeIn {
      from { opacity: 0; transform: translateY(-10px); }
      to { opacity: 1; transform: translateY(0); }
    }
    
    @media (max-width: 768px) {
      .publication-title { font-size: 2.2rem; }
      .hero-subtitle { font-size: 1.2rem; }
      .section-header h2 { font-size: 1.8rem; }
      .publication-links { flex-direction: column; align-items: center; }
      .btn { width: 100%; max-width: 320px; }
      .ps-container { flex-direction: column; }
      .dropdown-content {
        position: static;
        margin-top: 10px;
        width: 100%;
        max-width: 320px;
        box-shadow: none;
        border: 1px solid var(--border);
        display: none; /* Ensure it's hidden by default on mobile too */
      }
      .dropdown-content.show {
        display: block; /* JS will handle showing it */
      }
      .dropdown-content::before { display: none; }
    }
    
    @media (max-width: 480px) {
      .publication-title { font-size: 2rem; }
      .authors, .affiliations { font-size: 0.9rem; }
    }
  </style>
</head>
<body>

<!-- Hero Section -->
<section class="hero">
  <div class="container">
    <div class="hero-body">
      <h1 class="publication-title">HQ-CLIP</h1>
      <h2 class="hero-subtitle">Leveraging Large Vision-Language Models to Create High-Quality Image-Text Datasets and CLIP Models</h2>

      <div class="authors">
        <div class="author-block">Zhixiang Wei<sup>1,2,*</sup></div>
        <div class="author-block">Guangting Wang<sup>2,*</sup></div>
        <div class="author-block">Xiaoxiao Ma<sup>1</sup></div>
        <div class="author-block">Ke Mei<sup>2</sup></div>
        <div class="author-block">Huaian Chen<sup>1</sup></div>
        <div class="author-block">Yi Jin<sup>1</sup></div>
        <div class="author-block">Fengyun Rao<sup>2</sup></div>
      </div>

      <div class="affiliations">
        <div><sup>1</sup>University of Science and Technology of China</div>
        <div><sup>2</sup>WeChat Vision, Tencent Inc.</div>
        <div><sup>*</sup>Equal Contribution</div>
        <div class="conference">ICCV 2025</div>
      </div>

      <div class="divider"></div>

      <div class="publication-links">
        <a href="https://arxiv.org/abs/xxxx" class="btn btn-primary">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor"><path d="M15 4H5V20H19V8H15V4ZM5 2H16L20 6V20C20 21.1046 19.1046 22 18 22H6C4.89543 22 4 21.1046 4 20V4C4 2.89543 4.89543 2 6 2H5ZM14 9V5L18 9H14Z"></path></svg>
          <span>Paper</span>
        </a>
        <a href="https://github.com/w1oves/hqclip" class="btn">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor"><path d="M12 2C6.475 2 2 6.475 2 12C2 16.425 4.8625 20.1625 8.8375 21.4875C9.3375 21.575 9.525 21.275 9.525 21.0125C9.525 20.775 9.5125 20.0125 9.5125 19.2C6.7375 19.7125 5.9125 17.7625 5.9125 17.7625C5.4625 16.6375 4.6875 16.3375 4.6875 16.3375C3.7875 15.725 4.7625 15.7375 4.7625 15.7375C5.75 15.8125 6.35 16.7625 6.35 16.7625C7.225 18.25 8.7125 17.8 9.3 17.5375C9.3875 16.9125 9.6375 16.525 9.8875 16.3125C7.65 16.0625 5.2875 15.2 5.2875 11.475C5.2875 10.3875 5.675 9.5125 6.3 8.8375C6.2 8.5875 5.8625 7.55 6.4 6.4625C6.4 6.4625 7.2125 6.2 9.525 7.7C10.2875 7.5 11.15 7.4 12 7.4C12.85 7.4 13.7125 7.5 14.475 7.7C16.7875 6.2 17.6 6.4625 17.6 6.4625C18.1375 7.55 17.8 8.5875 17.7 8.8375C18.325 9.5125 18.7125 10.3875 18.7125 11.475C18.7125 15.2125 16.3375 16.0625 14.1 16.3125C14.425 16.5875 14.725 17.125 14.725 18C14.725 19.2375 14.7125 20.2 14.7125 20.5125C14.7125 20.775 14.9 21.0875 15.4125 21.4875C19.1375 20.1625 22 16.425 22 12C22 6.475 17.525 2 12 2Z"></path></svg>
          <span>Code</span>
        </a>
        <a href="https://huggingface.co/spaces/zhixiangwei/hqclip" class="btn">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor"><path d="M21.6521 11.121L18.2161 3.33303C17.9361 2.69603 17.2881 2.25003 16.5711 2.25003H7.42811C6.71111 2.25003 6.06311 2.69603 5.78311 3.33303L2.34711 11.121C2.13311 11.62 2.13211 12.17 2.34411 12.668L5.78411 20.667C6.06411 21.304 6.71211 21.75 7.42911 21.75H16.5711C17.2881 21.75 17.9361 21.304 18.2161 20.667L21.6521 12.879C21.8671 12.38 21.8671 11.62 21.6521 11.121ZM16.5711 19.5H7.42811L4.55311 12.913L6.42811 12L11.0001 14.25V6.75L6.42811 9L4.55311 7.08703L7.42811 4.5H16.5711L19.4461 11.087L17.5711 12L13.0001 9.75V17.25L17.5711 15L19.4461 16.913L16.5711 19.5Z"></path></svg>
          <span>Demo</span>
        </a>
        
        <div class="dropdown">
          <button class="btn dropdown-btn">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor"><path d="M20 7V17C20 17.5523 19.5523 18 19 18H5C4.44772 18 4 17.5523 4 17V7C4 6.44772 4.44772 6 5 6H19C19.5523 6 20 6.44772 20 7ZM18 4H6C4.89543 4 4 4.89543 4 6V8H2V6C2 3.79086 3.79086 2 6 2H18C20.2091 2 22 3.79086 22 6V8H20V6C20 4.89543 19.1046 4 18 4ZM2 18V16H4V18C4 19.1046 4.89543 20 6 20H18C19.1046 20 20 19.1046 20 18V16H22V18C22 20.2091 20.2091 22 18 22H6C3.79086 22 2 20.2091 2 18Z"></path></svg>
            <span>Datasets</span>
          </button>
          <div class="dropdown-content">
            <a href="https://huggingface.co/datasets/zhixiangwei/VLM-150M">VLM-150M (147M samples)</a>
            <a href="https://huggingface.co/datasets/zhixiangwei/VLM-1B">VLM-1B</a>
          </div>
        </div>
        
        <div class="dropdown">
          <button class="btn dropdown-btn">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor"><path d="M12 2C13.1046 2 14 2.89543 14 4V6.14286C16.8043 6.54843 19 8.9995 19 12C19 14.9995 16.8049 17.451 14 17.8571V20C14 21.1046 13.1046 22 12 22C10.8954 22 10 21.1046 10 20V17.8571C7.19511 17.451 5 14.9995 5 12C5 8.9995 7.1957 6.54843 10 6.14286V4C10 2.89543 10.8954 2 12 2ZM12 8C9.79086 8 8 9.79086 8 12C8 14.2091 9.79086 16 12 16C14.2091 16 16 14.2091 16 12C16 9.79086 14.2091 8 12 8Z"></path></svg>
            <span>Models</span>
          </button>
          <div class="dropdown-content">
            <a href="https://huggingface.co/zhixiangwei/vlm150m-hqclip-large-vitb16">CLIP-B-16 (VLM-150M)</a>
            <a href="https://huggingface.co/zhixiangwei/vlm1b-hqclip-xlarge-vitl14-clipa">CLIP-L-14-CLIPA (VLM-1B)</a>
            <a href="https://huggingface.co/zhixiangwei/hqclip-openai-large-ft-vlm1b">CLIP-L-14-OPENAI (VLM-1B)</a>
            <a href="https://huggingface.co/zhixiangwei/qwen2-7b-full">Recaption Model (Qwen2VL)</a>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

  <!-- Abstract Section -->
  <section>
    <div class="container">
      <div class="section-header">
        <h2>Abstract</h2>
      </div>
      <div class="content">
        <p>Large-scale but noisy image-text pair data have paved the way for the success of Contrastive Language-Image Pretraining (CLIP). As the foundation vision encoder, CLIP in turn serves as the cornerstone for most large vision-language models (LVLMs). This interdependence naturally raises an interesting question: Can we reciprocally leverage LVLMs to enhance the quality of image-text pair data, thereby opening the possibility of a self-reinforcing cycle for continuous improvement? In this work, we take a significant step toward this vision by introducing an LVLM-driven data refinement pipeline.</p>
        <p>Our framework leverages LVLMs to process images and their raw alt-text, generating <span class="highlight">four complementary textual formulas</span>: long positive descriptions, long negative descriptions, short positive tags, and short negative tags. Applying this pipeline to the curated DFN-Large dataset yields <span class="highlight">VLM-150M</span>, a refined dataset enriched with multi-grained annotations. Based on this dataset, we further propose a training paradigm that extends conventional contrastive learning by incorporating negative descriptions and short tags as additional supervised signals.</p>
        <p>The resulting model, <span class="highlight">HQ-CLIP</span>, demonstrates remarkable improvements across diverse benchmarks. Within a comparable training data scale, our approach achieves <span class="highlight"></span>state-of-the-art performance</span> in zero-shot classification, cross-modal retrieval, and fine-grained visual understanding tasks. In retrieval benchmarks, HQ-CLIP even surpasses standard CLIP models trained on the DFN-2B dataset, which contains 10x more training data than ours.</p>
      </div>
    </div>
  </section>

  <!-- Teaser Image Section -->
  <section style="padding-top: 0;">
    <div class="container">
        <div class="full-width-image-container">
            <img src="static/teaser.jpg" alt="A visual comparison showing how HQ-CLIP refines noisy web data into high-quality, multi-grained image-text pairs.">
            <p class="image-caption">HQ-CLIP leverages advanced LVLMs to transform noisy, raw image-text pairs from the web into a high-quality dataset with rich, multi-grained annotations, significantly boosting model performance.</p>
        </div>
    </div>
  </section>

  <!-- Problem and Solution Section -->
  <section class="problem-solution">
    <div class="container">
      <div class="section-header">
        <h2>Research Challenge</h2>
      </div>
      <div class="ps-container">
        <div class="ps-item">
          <h3 class="ps-title">Problem</h3>
          <div class="content" style="max-width: none;">
            <p>Existing methods for enhancing CLIP with LLMs suffer from limitations:</p>
            <ul style="padding-left: 1.5rem; margin-top: 1rem;">
              <li>Single-modality approaches (LaCLIP, WhatIf) neglect cross-modal correlations</li>
              <li>Hybrid methods (CapFusion, VeCLIP) introduce computational complexity</li>
              <li>Cascade pipelines cause potential error propagation</li>
              <li>Information asymmetry between visual and textual modalities</li>
              <li>High computational cost of using SoTA close-source LVLMs at scale</li>
            </ul>
          </div>
        </div>
        <div class="ps-item">
          <h3 class="ps-title">Our Solution</h3>
          <div class="content" style="max-width: none;">
            <p>HQ-CLIP introduces a unified approach:</p>
            <ul style="padding-left: 1.5rem; margin-top: 1rem;">
              <li>Single LVLM processes both images and paired texts simultaneously</li>
              <li>Generates <span class="highlight">four complementary descriptions</span>: long/short positives and negatives</li>
              <li>Cost-efficient pipeline: SFT on compact LVLMs after GPT-4o curation</li>
              <li>Extension of contrastive learning with two innovations:
                <ul style="padding-left: 1.5rem;">
                  <li>Short-Tag Classification (STC) loss</li>
                  <li>Hard Negative Identification (HNI) mechanism</li>
                </ul>
              </li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Approach Section -->
  <section>
    <div class="container">
      <div class="section-header">
        <h2>Approach Overview</h2>
      </div>
      <div class="content">
        <p>Our approach consists of two main stages: an LVLM-driven data refinement pipeline to generate high-quality, multi-grained annotations, and a novel training paradigm, HQ-CLIP, that effectively utilizes this enriched data. The entire framework is designed for efficiency and scalability.</p>
      </div>
      <div class="full-width-image-container">
        <img src="static/framework.jpg" alt="Diagram of the HQ-CLIP framework, showing the data refinement pipeline and the training process.">
        <p class="image-caption">The overall framework of HQ-CLIP. We first use a fine-tuned LVLM to process raw image-text pairs into four types of textual descriptions. Then, we train the CLIP model using these multi-grained signals, incorporating Hard Negative Identification (HNI) and Short-Tag Classification (STC).</p>
      </div>
      <div class="methodology-items">
        <div class="method-item">
          <div class="method-icon">üìú</div>
          <h3 class="method-title">Long Positive Descriptions</h3>
          <p>Detailed, contextual descriptions aligned with the image content, providing richer information over raw text data.</p>
        </div>
        <div class="method-item">
          <div class="method-icon">üö´</div>
          <h3 class="method-title">Long Negative Descriptions</h3>
          <p>Contradictory descriptions used in Hard Negative Identification (HNI) to strengthen CLIP's ability to discern subtle discrepancies.</p>
        </div>
        <div class="method-item">
          <div class="method-icon">üè∑Ô∏è</div>
          <h3 class="method-title">Short Positive Tags</h3>
          <p>Concise categorical tags enabling Short-Tag Classification (STC) for discrete classification targets.</p>
        </div>
        <div class="method-item">
          <div class="method-icon">‚õî</div>
          <h3 class="method-title">Short Negative Tags</h3>
          <p>Contrastive tags providing fine-grained discriminative signals for visual-textual alignment.</p>
        </div>
      </div>
    </div>
  </section>

  <!-- Results Section -->
  <section class="results-section">
    <div class="container">
      <div class="section-header">
        <h2>Experimental Results</h2>
      </div>
      <div class="content">
        <p>We evaluated HQ-CLIP on a comprehensive set of 38 benchmark datasets. Our method demonstrates state-of-the-art performance in zero-shot classification and cross-modal retrieval, significantly outperforming models trained on comparable or even much larger datasets. The benefits also extend to downstream tasks, where HQ-CLIP serves as a superior vision backbone for LVLMs like LLaVA-1.5.</p>
      </div>
      
      <div class="results-container">
        <div class="result-block">
            <h3 class="result-title">Comparison with State-of-the-Art Models</h3>
            <p class="result-description">HQ-CLIP achieves top performance on zero-shot ImageNet classification and cross-modal retrieval benchmarks (Flickr30K, MSCOCO), surpassing previous methods trained on similar data scales.</p>
            <div class="full-width-image-container">
                <img src="static/table2.png" alt="Table comparing HQ-CLIP with SoTA models on zero-shot and retrieval tasks.">
            </div>
        </div>

        <div class="result-block">
            <h3 class="result-title">Ablation Studies</h3>
            <p class="result-description">Our ablation studies validate the effectiveness of VLM-150M and HQ-CLIP.</p>
            <div class="full-width-image-container">
                <img src="static/table34.png" alt="Table showing ablation study results for different components of HQ-CLIP.">
            </div>
        </div>

        <div class="result-block">
            <h3 class="result-title">Performance as an LVLM Visual Encoder</h3>
            <p class="result-description">When used as the vision backbone for LLaVA-1.5, our pre-trained HQ-CLIP leads to improved performance compared to the other CLIP backbones.</p>
            <div class="full-width-image-container">
                <img src="static/table5.png" alt="Table showing the performance of HQ-CLIP as a visual encoder in LLaVA-1.5.">
            </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Contributions Section -->
  <section class="contributions">
    <div class="container">
      <div class="section-header">
        <h2>Key Contributions</h2>
      </div>
      <div class="content">
        <ul class="contrib-list">
          <li>We introduce an <span class="highlight">efficient and effective LVLM-driven data refinement pipeline</span> and apply it to DFN-Large, creating <span class="highlight">VLM-150M</span>, a high-quality dataset with multi-grained descriptions.</li>
          <li>We propose <span class="highlight">HQ-CLIP</span>, a specialized framework that combines Hard Negative Identification (HNI) for fine-grained understanding and Short-Tag Classification (STC) for categorical semantic recognition.</li>
          <li>Through large-scale experiments, HQ-CLIP demonstrates <span class="highlight">state-of-the-art zero-shot generalization</span> and exceptional cross-modal retrieval capabilities, surpassing models trained on 10x more data.</li>
          <li>When deployed as the visual backbone for LLaVA-1.5, HQ-CLIP <span class="highlight">outperforms other ViT-B architectures</span>, showcasing its potential as a superior vision encoder for LVLMs.</li>
        </ul>
      </div>
    </div>
  </section>

  <!-- Poster Section (MOVED) -->
  <section id="poster-section">
    <div class="container">
      <div class="section-header">
        <h2>Poster</h2>
      </div>
      <div class="full-width-image-container">
        <img src="static/posters.jpg" alt="Conference poster for the HQ-CLIP paper">
        <p class="image-caption">Official conference poster for HQ-CLIP presented at ICCV 2025.</p>
      </div>
    </div>
  </section>

  <!-- BibTeX Section -->
  <section class="bibtex-section">
    <div class="container">
      <div class="section-header">
        <h2>Citation</h2>
      </div>
      <div class="bibtex-code">
@inproceedings{wei2025hqclip,
  title     = {{HQ-CLIP}: Leveraging Large Vision-Language Models to Create High-Quality Image-Text Datasets and {CLIP} Models},
  author    = {Wei, Zhixiang and Wang, Guangting and Ma, Xiaoxiao and Mei, Ke and Chen, Huaian and Jin, Yi and Rao, Fengyun},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  year      = {2025}
}</div>
    </div>
  </section>

  <!-- Footer -->
  <footer>
    <div class="container">
      <div class="footer-text">
        <p>This project is part of research conducted at University of Science and Technology of China and WeChat Vision, Tencent Inc. All code, models and datasets will be released upon publication.</p>
        <p class="copyright">¬© 2025 HQ-CLIP Project Team | ICCV 2025</p>
      </div>
    </div>
  </footer>

  <!-- JavaScript for Dropdown Menu -->
  <script>
    document.addEventListener('DOMContentLoaded', function () {
      const dropdowns = document.querySelectorAll('.dropdown');

      // Function to close all open dropdowns
      function closeAllDropdowns() {
        dropdowns.forEach(dropdown => {
          const content = dropdown.querySelector('.dropdown-content');
          const button = dropdown.querySelector('.dropdown-btn');
          if (content.classList.contains('show')) {
            content.classList.remove('show');
            button.classList.remove('active');
          }
        });
      }

      dropdowns.forEach(dropdown => {
        const button = dropdown.querySelector('.dropdown-btn');
        const content = dropdown.querySelector('.dropdown-content');

        button.addEventListener('click', function (event) {
          // Prevent the window click event from closing the menu immediately
          event.stopPropagation();
          
          // Check if this dropdown is already open
          const isAlreadyOpen = content.classList.contains('show');

          // First, close all dropdowns
          closeAllDropdowns();

          // If it wasn't open, open it now
          if (!isAlreadyOpen) {
            content.classList.add('show');
            button.classList.add('active');
          }
        });
      });

      // Add a click listener to the window to close dropdowns when clicking outside
      window.addEventListener('click', function () {
        closeAllDropdowns();
      });
    });
  </script>

</body>
</html>