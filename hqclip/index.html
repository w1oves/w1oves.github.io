<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>[ICCV 2025] HQ-CLIP: Leveraging Large Vision-Language Models to Create High-Quality Image-Text Datasets üöÄ</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style>
    body { font-family: 'Segoe UI', Arial, sans-serif; margin: 0; padding: 0; background: #f9f9fb; color: #222; }
    .container { max-width: 900px; margin: 0 auto; padding: 32px 16px 64px 16px; background: #fff; box-shadow: 0 2px 8px rgba(0,0,0,0.04);}
    h1 { font-size: 2.2em; margin-bottom: 0.2em; }
    h2 { border-bottom: 2px solid #eee; padding-bottom: 0.2em; margin-top: 2em; }
    h3 { margin-top: 1.5em; }
    .badges { margin-bottom: 1.5em; }
    .badges a { margin-right: 8px; vertical-align: middle; }
    .authors { margin-bottom: 1.5em; font-size: 1.1em; }
    .authors sup { font-size: 0.8em; }
    .affiliations { color: #666; font-size: 0.98em; margin-bottom: 2em; }
    ul.keypoints { list-style: none; padding-left: 0; }
    ul.keypoints li { margin-bottom: 1em; padding-left: 2em; position: relative; }
    ul.keypoints li:before { content: "‚Ä¢"; position: absolute; left: 0; color: #0072c6; font-size: 1.3em; top: 0.1em;}
    .model-overview { display: block; margin: 2em auto 2em auto; max-width: 100%; border-radius: 8px; box-shadow: 0 1px 6px rgba(0,0,0,0.07);}
    table { border-collapse: collapse; width: 100%; margin: 1.5em 0; background: #fcfcfc;}
    th, td { border: 1px solid #e0e0e0; padding: 8px 12px; text-align: center; }
    th { background: #f2f6fa; font-weight: 600;}
    tr:nth-child(even) { background: #f7f9fa;}
    .img-table { display: block; margin: 2em auto; max-width: 100%; border-radius: 6px; box-shadow: 0 1px 4px rgba(0,0,0,0.05);}
    .section-note { color: #888; font-size: 0.98em; margin-bottom: 1em;}
    .citation { background: #f6f8fa; border-left: 4px solid #0072c6; padding: 1em; margin: 2em 0;}
    .acknowledgments { background: #f9f9fb; border-left: 4px solid #e0e0e0; padding: 1em; margin: 2em 0;}
    .footnote { color: #888; font-size: 0.95em;}
    a { color: #0072c6; text-decoration: none; }
    a:hover { text-decoration: underline; }
    @media (max-width: 600px) {
      .container { padding: 12px 2vw; }
      h1 { font-size: 1.3em; }
      table, th, td { font-size: 0.96em; }
    }
  </style>
</head>
<body>
  <div class="container">
    <h1>
      [ICCV 2025] HQ-CLIP: Leveraging Large Vision-Language Models to Create High-Quality Image-Text Datasets üöÄ
    </h1>
    <div class="badges">
      <a href="https://arxiv.org/pdf/xxxx.xxxx.pdf"><img src="https://img.shields.io/badge/üìÑ_Paper-PDF-critical?logo=adobeacrobatreader" alt="Paper PDF"></a>
      <a href="https://zxwei.site/hqclip/"><img src="https://img.shields.io/badge/üåê_Project-Page-blue?logo=googlechrome" alt="Project Page"></a>
      <a href="https://huggingface.co/spaces/zhixiangwei/hqclip"><img src="https://img.shields.io/badge/ü§ó_Demo-HuggingFace_Spaces-yellow" alt="Demo"></a>
      <a href="https://huggingface.co/datasets/zhixiangwei/VLM-150M"><img src="https://img.shields.io/badge/üìÄ_Dataset-VLM--150M-brightgreen" alt="Dataset VLM-150M"></a>
      <a href="https://huggingface.co/datasets/zhixiangwei/VLM-1B"><img src="https://img.shields.io/badge/üíø_Dataset-VLM--1B-success" alt="Dataset VLM-1B"></a>
    </div>
    <div class="authors">
      <strong>Authors</strong><br>
      <a href="https://zxwei.site">Zhixiang Wei</a><sup>1</sup>, Guangting Wang <em>et al.</em>
    </div>
    <div class="affiliations">
      <sup>1</sup> University of Science and Technology of China<br>
      <sup>2</sup> WeChat Vision, Tencent Inc.
    </div>

    <h2>üîç Key Contributions</h2>
    <ul class="keypoints">
      <li>
        <b>üè≠ Efficient Data Generation Pipeline</b><br>
        Multi-grained annotation pipeline using Large Vision-Language Models (LVLMs)
      </li>
      <li>
        <b>üóÇÔ∏è High-Quality Image-Text Datasets</b><br>
        Generated by state-of-the-art LVLMs with positive/negative examples and rich text descriptions:<br>
        &nbsp;&nbsp;‚Ä¢ <a href="https://huggingface.co/datasets/zhixiangwei/VLM-1B"><b>VLM-1B</b></a>: Billion-scale dataset<br>
        &nbsp;&nbsp;‚Ä¢ <a href="https://huggingface.co/datasets/zhixiangwei/VLM-150M"><b>VLM-150M</b></a>: Curated 150M samples
      </li>
      <li>
        <b>üß† HQ-CLIP Training Framework</b><br>
        Novel CLIP training paradigm extending contrastive learning with:<br>
        &nbsp;&nbsp;‚Ä¢ Negative description supervision<br>
        &nbsp;&nbsp;‚Ä¢ Short tag augmentation
      </li>
    </ul>

    <img class="model-overview" src="https://github.com/user-attachments/assets/e700f75b-e0a5-4328-8466-6b496a4f971d" alt="Model Overview">

    <h2>Model Zoo</h2>
    <table>
      <tr>
        <th>Model</th>
        <th>Pretrained</th>
        <th>ImageNet Top-1</th>
        <th>DataComp Score</th>
      </tr>
      <tr>
        <td><a href="https://huggingface.co/zhixiangwei/vlm150m-hqclip-large-vitb16">CLIP-B-16</a></td>
        <td>VLM-150M-Medium</td>
        <td>70.6</td>
        <td>58.6</td>
      </tr>
      <tr>
        <td><a href="https://huggingface.co/zhixiangwei/vlm1b-hqclip-xlarge-vitl14-clipa">CLIP-L-14-CLIPA</a></td>
        <td>VLM-1B</td>
        <td>78.6</td>
        <td>63.8</td>
      </tr>
      <tr>
        <td><a href="https://huggingface.co/zhixiangwei/hqclip-openai-large-ft-vlm1b">CLIP-L-14-OPENAI</a></td>
        <td>VLM-1B</td>
        <td>76.5</td>
        <td>63.7</td>
      </tr>
    </table>
    <div class="section-note">
      Recaption Model: <a href="https://huggingface.co/zhixiangwei/qwen2-7b-full">Qwen2VL</a>
    </div>

    <h2>Datasets</h2>
    <table>
      <tr>
        <th>Dataset</th>
        <th>Samples</th>
        <th>URL</th>
      </tr>
      <tr>
        <td>VLM-150M</td>
        <td>147M</td>
        <td><a href="https://huggingface.co/datasets/zhixiangwei/VLM-150M">Link</a></td>
      </tr>
      <tr>
        <td>VLM-1B</td>
        <td>-</td>
        <td><a href="https://huggingface.co/datasets/zhixiangwei/VLM-1B">Link</a></td>
      </tr>
    </table>

    <h2>Dataset Usage Guide</h2>
    <h3>Preparation Steps</h3>
    <ol>
      <li>
        <b>(Optional) Download CommonPool Foundation Datasets</b><br>
        Access CommonPool Large and XLarge versions via:<br>
        <a href="https://github.com/mlfoundations/datacomp#downloading-commonpool">CommonPool GitHub Repository</a>
      </li>
      <li>
        <b>Acquire DFN Base Datasets</b><br>
        Download DFN Large and XLarge from:<br>
        <a href="https://huggingface.co/datasets/apf1/datafilteringnetworks_2b">DFN Hugging Face Datasets</a>
      </li>
      <li>
        <b>Download HQ-CLIP Datasets</b><br>
        Obtain our enhanced datasets:<br>
        &nbsp;&nbsp;‚Ä¢ VLM-150M<br>
        &nbsp;&nbsp;‚Ä¢ VLM-1B
      </li>
    </ol>
    <h3>Integration Instructions</h3>
    <p>
      Each JSON entry in VLM-150M and VLM-1B corresponds directly to a DFN dataset UID through matching filenames. To utilize our enhanced annotations:
    </p>
    <ul>
      <li>
        <b>Option 1: Direct Caption Replacement</b><br>
        Substitute the original DFN captions with our generated annotations
      </li>
      <li>
        <b>Option 2: Dynamic Data Loading</b><br>
        Modify the Open CLIP dataloader to load our annotations during training runtime
      </li>
    </ul>
    <p class="section-note">üîú Detailed implementation guidance will be published in future releases.</p>

    <h2>üß™ Experimental Results</h2>
    <h3>Comparison with SoTA Models</h3>
    <img class="img-table" src="table2.png" alt="Comparison with SoTA models (Table 2)">
    <h3>Performance as LVLM Visual Encoder</h3>
    <img class="img-table" src="table5.png" alt="Performance as LVLM Visual Encoder (Table 5)">
    <h3>Ablation Study</h3>
    <img class="img-table" src="table34.png" alt="Ablation Study (Table 3/4)">

    <h2>üìù Citation</h2>
    <div class="citation">
      <pre>
@InProceedings{Wei2025HQCLIP,
  title     = {HQ-CLIP: Leveraging Large Vision-Language Models to Create High-Quality Image-Text Datasets},
  author    = {Wei, Zhixiang and Wang, Guangting and et al.},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
  year      = {2025}
}
      </pre>
    </div>

    <h2>üôè Acknowledgments</h2>
    <div class="acknowledgments">
      These works have greatly inspired us, providing us with codebases, data, and support. We thank their authors!
      <ul>
        <li><a href="https://github.com/mlfoundations/open_clip.git">open_clip</a></li>
        <li><a href="https://github.com/LAION-AI/CLIP_benchmark">clip_benchmark</a></li>
        <li><a href="https://github.com/mlfoundations/datacomp">datacomp</a></li>
        <li><a href="https://huggingface.co/collections/apple/dfn-models-data-659ecf85cebd98088a9d9a3b">DFN</a></li>
        <li><a href="https://huggingface.co/datasets/UCSC-VLAA/Recap-DataComp-1B">What If</a></li>
      </ul>
    </div>
  </div>
</body>
</html>