<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>[ICCV 2025] HQ-CLIP: Enhancing CLIP with Large Vision-Language Models</title>
  
  <!-- Meta Tags -->
  <meta name="description" content="[ICCV 2025] HQ-CLIP leverages LVLMs to create high-quality image-text datasets and enhance CLIP models through multi-grained supervision">
  <meta name="keywords" content="HQ-CLIP, CLIP, Vision-Language Models, Multi-modal Learning, Dataset Refinement">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Social Sharing -->
  <meta property="og:title" content="HQ-CLIP: Creating High-Quality Image-Text Datasets with LVLMs"/>
  <meta property="og:image" content="https://zxwei.site/images/iccv25.png"/>
  <meta property="og:url" content="https://zxwei.site/hqclip"/>
  
  <!-- Fonts -->
  <link href="https://fonts.googleapis.com/css2?family=Libre+Baskerville:wght@400;700&family=Roboto:wght@300;400;500;700&display=swap" rel="stylesheet">

  <!-- Styles -->
  <style>
    :root {
      --primary: #2c5e92;
      --secondary: #4a7bab;
      --accent: #e63946;
      --light: #f8f9fa;
      --dark: #212529;
      --border: #dee2e6;
      --shadow: rgba(0,0,0,0.1);
    }
    
    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }
    
    body {
      font-family: 'Roboto', sans-serif;
      color: #333;
      line-height: 1.6;
      background-color: #fff;
      overflow-x: hidden;
    }
    
    /* Header Section */
    .hero {
      background: linear-gradient(135deg, var(--primary) 0%, var(--secondary) 100%);
      color: white;
      padding: 3rem 1rem;
      position: relative;
      overflow: hidden;
    }
    
    .hero::before {
      content: "";
      position: absolute;
      top: 0;
      left: 0;
      right: 0;
      bottom: 0;
      background: url('data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxMDAlIiBoZWlnaHQ9IjEwMCUiPjxkZWZzPjxwYXR0ZXJuIGlkPSJwYXR0ZXJuIiB3aWR0aD0iNDAiIGhlaWdodD0iNDAiIHBhdHRlcm5Vbml0cz0idXNlclNwYWNlT25Vc2UiIHBhdHRlcm5UcmFuc2Zvcm09InJvdGF0ZSg0NSkiPjxyZWN0IHg9IjAiIHk9IjAiIHdpZHRoPSIyMCIgaGVpZ2h0PSIyMCIgZmlsbD0icmdiYSgyNTUsIDI1NSwgMjU1LCAwLjA1KSIgLz48L3BhdHRlcm4+PC9kZWZzPjxyZWN0IHdpZHRoPSIxMDAlIiBoZWlnaHQ9IjEwMCUiIGZpbGw9InVybCgjcGF0dGVybikiLz48L3N2Zz4=');
      opacity: 0.15;
    }
    
    .container {
      max-width: 1200px;
      margin: 0 auto;
      padding: 0 1.5rem;
      position: relative;
      z-index: 2;
    }
    
    .hero-body {
      padding: 2rem 0;
    }
    
    .publication-title {
      font-family: 'Libre Baskerville', serif;
      font-size: 2.5rem;
      font-weight: 700;
      margin-bottom: 1rem;
      text-shadow: 0 2px 4px rgba(0,0,0,0.2);
      line-height: 1.2;
    }
    
    .hero-subtitle {
      font-size: 1.5rem;
      font-weight: 300;
      margin-bottom: 2rem;
      max-width: 800px;
      margin-left: auto;
      margin-right: auto;
    }
    
    .authors {
      font-size: 1.1rem;
      margin-bottom: 1.5rem;
      text-shadow: 0 1px 2px rgba(0,0,0,0.2);
    }
    
    .author-block {
      display: inline-block;
      margin: 0 8px 5px 0;
      padding: 0 8px;
    }
    
    .affiliations {
      font-size: 0.95rem;
      margin-bottom: 1.5rem;
      max-width: 800px;
      margin-left: auto;
      margin-right: auto;
    }
    
    .divider {
      height: 1px;
      background: rgba(255,255,255,0.2);
      margin: 1.5rem auto;
      max-width: 600px;
    }
    
    .publication-links {
      display: flex;
      justify-content: center;
      flex-wrap: wrap;
      gap: 15px;
      margin-top: 2rem;
      position: relative;
      z-index: 100;
    }
    
    .btn {
      display: flex;
      align-items: center;
      justify-content: center;
      padding: 12px 20px;
      background: rgba(255,255,255,0.9);
      color: var(--primary);
      border-radius: 6px;
      text-decoration: none;
      font-weight: 500;
      transition: all 0.3s ease;
      box-shadow: 0 4px 6px rgba(0,0,0,0.1);
      text-align: center;
      min-width: 160px;
      position: relative;
    }
    
    .btn:hover {
      transform: translateY(-3px);
      box-shadow: 0 6px 12px rgba(0,0,0,0.15);
      background: white;
      color: var(--accent);
    }
    
    .btn-primary {
      background: white;
      color: var(--accent);
      font-weight: 600;
    }
    
    .btn .icon {
      margin-right: 10px;
      font-size: 1.2em;
    }
    
    /* Main Content Sections */
    section {
      padding: 4rem 0;
      position: relative;
      z-index: 10;
    }
    
    .section-header {
      text-align: center;
      margin-bottom: 3rem;
      position: relative;
      padding-bottom: 1rem;
    }
    
    .section-header h2 {
      font-family: 'Libre Baskerville', serif;
      font-size: 2.2rem;
      color: var(--primary);
      position: relative;
      display: inline-block;
      padding: 0 20px;
    }
    
    .section-header h2::after {
      content: "";
      position: absolute;
      left: 0;
      bottom: -10px;
      width: 100%;
      height: 3px;
      background: var(--accent);
    }
    
    .content {
      font-size: 1.05rem;
      line-height: 1.8;
      color: #444;
    }
    
    .content p {
      margin-bottom: 1.5rem;
    }
    
    .highlight {
      background: linear-gradient(120deg, rgba(230,57,70,0.1), rgba(230,57,70,0));
      padding: 3px 6px;
      border-radius: 4px;
      font-weight: 500;
    }
    
    /* Problem-Solution Section */
    .problem-solution {
      background: var(--light);
    }
    
    .ps-container {
      display: flex;
      flex-wrap: wrap;
      gap: 3rem;
      margin-top: 2rem;
    }
    
    .ps-item {
      flex: 1;
      min-width: 300px;
      padding: 2rem;
      background: white;
      border-radius: 10px;
      box-shadow: 0 5px 15px var(--shadow);
      transition: transform 0.3s ease;
    }
    
    .ps-item:hover {
      transform: translateY(-10px);
    }
    
    .ps-title {
      font-size: 1.4rem;
      color: var(--primary);
      margin-bottom: 1.2rem;
      font-weight: 600;
      position: relative;
      padding-left: 20px;
    }
    
    .ps-title::before {
      content: "";
      position: absolute;
      left: 0;
      top: 50%;
      transform: translateY(-50%);
      height: 25px;
      width: 4px;
      background: var(--accent);
    }
    
    /* Approach Section */
    .approach-container {
      display: flex;
      flex-wrap: wrap;
      gap: 2.5rem;
      margin-top: 2rem;
    }
    
    .approach-text {
      flex: 1;
      min-width: 300px;
    }
    
    .approach-image {
      flex: 1;
      min-width: 300px;
      border-radius: 10px;
      overflow: hidden;
      box-shadow: 0 10px 30px rgba(0,0,0,0.15);
    }
    
    .approach-image img {
      width: 100%;
      height: auto;
      display: block;
      transition: transform 0.5s ease;
    }
    
    .approach-image img:hover {
      transform: scale(1.03);
    }
    
    .approach-caption {
      text-align: center;
      font-size: 0.9rem;
      color: #666;
      margin-top: 10px;
      font-style: italic;
    }
    
    /* Methodology Items */
    .methodology-items {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
      gap: 2rem;
      margin-top: 2rem;
    }
    
    .method-item {
      background: white;
      border-radius: 10px;
      padding: 2rem;
      box-shadow: 0 5px 15px var(--shadow);
      border-top: 4px solid var(--accent);
      transition: transform 0.3s ease;
    }
    
    .method-item:hover {
      transform: translateY(-5px);
    }
    
    .method-icon {
      font-size: 2.5rem;
      color: var(--accent);
      margin-bottom: 1rem;
    }
    
    .method-title {
      font-size: 1.3rem;
      color: var(--primary);
      margin-bottom: 1rem;
      font-weight: 600;
    }
    
    /* Results Section */
    .results-section {
      background: var(--light);
    }
    
    .results-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(350px, 1fr));
      gap: 2rem;
      margin-top: 2rem;
    }
    
    .results-item {
      background: white;
      border-radius: 10px;
      overflow: hidden;
      box-shadow: 0 5px 15px var(--shadow);
    }
    
    .result-image {
      height: 200px;
      overflow: hidden;
    }
    
    .result-image img {
      width: 100%;
      height: 100%;
      object-fit: cover;
      transition: transform 0.5s ease;
    }
    
    .result-image img:hover {
      transform: scale(1.05);
    }
    
    .result-content {
      padding: 1.5rem;
    }
    
    .result-title {
      font-size: 1.2rem;
      font-weight: 600;
      color: var(--primary);
      margin-bottom: 0.8rem;
    }
    
    /* Contributions Section */
    .contributions {
      background: linear-gradient(to right, var(--primary), var(--secondary));
      color: white;
    }
    
    .contributions .section-header h2 {
      color: white;
    }
    
    .contributions .content {
      color: rgba(255,255,255,0.9);
    }
    
    .contrib-list {
      list-style-type: none;
      max-width: 800px;
      margin: 0 auto;
    }
    
    .contrib-list li {
      margin-bottom: 1.5rem;
      padding-left: 2rem;
      position: relative;
      font-size: 1.1rem;
    }
    
    .contrib-list li::before {
      content: "‚úì";
      position: absolute;
      left: 0;
      top: 0;
      color: var(--accent);
      font-weight: bold;
    }
    
    /* BibTeX Section */
    .bibtex-section {
      background: var(--dark);
      color: white;
    }
    
    .bibtex-section .section-header h2 {
      color: white;
    }
    
    .bibtex-code {
      background: #2c3e50;
      padding: 2rem;
      border-radius: 8px;
      font-family: monospace;
      white-space: pre-wrap;
      overflow-x: auto;
      margin-top: 2rem;
      box-shadow: 0 10px 20px rgba(0,0,0,0.2);
      border-left: 4px solid var(--accent);
    }
    
    /* Footer */
    footer {
      background: var(--dark);
      color: rgba(255,255,255,0.7);
      padding: 3rem 0;
      text-align: center;
    }
    
    .footer-text {
      max-width: 700px;
      margin: 0 auto;
      font-size: 0.95rem;
      line-height: 1.6;
    }
    
    .copyright {
      margin-top: 1.5rem;
      font-size: 0.85rem;
      color: rgba(255,255,255,0.5);
    }
    
    /* Dropdown Styles */
    .dropdown {
      position: relative;
      display: inline-block;
    }
    
    .dropdown-content {
      display: none;
      position: absolute;
      background-color: white;
      min-width: 220px;
      box-shadow: 0 8px 16px rgba(0,0,0,0.2);
      z-index: 1000;
      border-radius: 6px;
      overflow: hidden;
      margin-top: 5px;
      left: 0;
      right: 0;
      animation: fadeIn 0.3s ease;
    }
    
    .dropdown-content a {
      color: #2c5e92;
      padding: 12px 16px;
      text-decoration: none;
      display: block;
      font-size: 0.95rem;
      transition: background-color 0.3s;
      border-bottom: 1px solid #f0f0f0;
    }
    
    .dropdown-content a:last-child {
      border-bottom: none;
    }
    
    .dropdown-content a:hover {
      background-color: #f0f7ff;
      color: #e63946;
    }
    
    .dropdown:hover .dropdown-content {
      display: block;
    }
    
    .dropdown-btn {
      display: flex;
      align-items: center;
      justify-content: center;
      padding: 12px 20px;
      background: rgba(255,255,255,0.9);
      color: var(--primary);
      border-radius: 6px;
      text-decoration: none;
      font-weight: 500;
      transition: all 0.3s ease;
      box-shadow: 0 4px 6px rgba(0,0,0,0.1);
      text-align: center;
      min-width: 160px;
      cursor: pointer;
      border: none;
      font-family: 'Roboto', sans-serif;
      font-size: 1rem;
    }
    
    .dropdown-btn:hover {
      transform: translateY(-3px);
      box-shadow: 0 6px 12px rgba(0,0,0,0.15);
      background: white;
      color: var(--accent);
    }
    
    .dropdown-btn .icon {
      margin-right: 10px;
      font-size: 1.2em;
    }
    
    .dropdown-btn::after {
      content: "‚ñº";
      margin-left: 8px;
      font-size: 0.7em;
    }
    
    @keyframes fadeIn {
      from { opacity: 0; transform: translateY(-10px); }
      to { opacity: 1; transform: translateY(0); }
    }
    
    /* Responsive Design */
    @media (max-width: 768px) {
      .publication-title {
        font-size: 2rem;
      }
      
      .hero-subtitle {
        font-size: 1.2rem;
      }
      
      .section-header h2 {
        font-size: 1.8rem;
      }
      
      .publication-links {
        flex-direction: column;
        align-items: center;
      }
      
      .btn, .dropdown-btn {
        width: 100%;
        max-width: 280px;
        margin-bottom: 10px;
      }
      
      .ps-container, .approach-container {
        flex-direction: column;
      }
      
      .dropdown-content {
        position: static;
        margin-top: 10px;
        width: 100%;
        box-shadow: none;
        border: 1px solid var(--border);
      }
      
      .dropdown:hover .dropdown-content {
        display: block;
      }
    }
    
    @media (max-width: 480px) {
      .publication-title {
        font-size: 1.8rem;
      }
      
      .hero-subtitle {
        font-size: 1.1rem;
      }
      
      .authors {
        font-size: 1rem;
      }
      
      .affiliations {
        font-size: 0.85rem;
      }
    }
  </style>
</head>
<body>

<!-- Hero Section -->
<section class="hero">
  <div class="container">
    <div class="hero-body">
      <h1 class="publication-title">HQ-CLIP</h1>
      <h2 class="hero-subtitle">Leveraging Large Vision-Language Models to Create High-Quality Image-Text Datasets and CLIP Models</h2>

      <div class="authors">
        <div class="author-block">Zhixiang Wei<sup>1,2,*</sup></div>
        <div class="author-block">Guangting Wang<sup>2,*</sup></div>
        <div class="author-block">Xiaoxiao Ma<sup>1</sup></div>
        <div class="author-block">Ke Mei<sup>2</sup></div>
        <div class="author-block">Huaian Chen<sup>1</sup></div>
        <div class="author-block">Yi Jin<sup>1</sup></div>
        <div class="author-block">Fengyun Rao<sup>2</sup></div>
      </div>

      <div class="affiliations">
        <div><sup>1</sup>University of Science and Technology of China</div>
        <div><sup>2</sup>WeChat Vision, Tencent Inc.</div>
        <div><sup>*</sup>Equal Contribution</div>
        <div class="conference">ICCV 2025</div>
      </div>

      <div class="divider"></div>

      <div class="publication-links">
        <a href="https://arxiv.org/abs/xxxx" class="btn btn-primary">
          <span class="icon">üìÑ</span> Paper
        </a>
        <a href="https://github.com/w1oves/hqclip" class="btn">
          <span class="icon">üíª</span> Code
        </a>
        <a href="https://huggingface.co/spaces/zhixiangwei/hqclip" class="btn">
          <span class="icon">üöÄ</span> Demo
        </a>
        
        <!-- Êï∞ÊçÆÈõÜÈìæÊé• -->
        <div class="dropdown">
          <button class="dropdown-btn">
            <span class="icon">üìä</span> Datasets
          </button>
          <div class="dropdown-content">
            <a href="https://huggingface.co/datasets/zhixiangwei/VLM-150M">VLM-150M (147M samples)</a>
            <a href="https://huggingface.co/datasets/zhixiangwei/VLM-1B">VLM-1B</a>
          </div>
        </div>
        
        <!-- Ê®°ÂûãÈìæÊé• -->
        <div class="dropdown">
          <button class="dropdown-btn">
            <span class="icon">üß†</span> Models
          </button>
          <div class="dropdown-content">
            <a href="https://huggingface.co/zhixiangwei/vlm150m-hqclip-large-vitb16">CLIP-B-16 (VLM-150M)</a>
            <a href="https://huggingface.co/zhixiangwei/vlm1b-hqclip-xlarge-vitl14-clipa">CLIP-L-14-CLIPA (VLM-1B)</a>
            <a href="https://huggingface.co/zhixiangwei/hqclip-openai-large-ft-vlm1b">CLIP-L-14-OPENAI (VLM-1B)</a>
            <a href="https://huggingface.co/zhixiangwei/qwen2-7b-full">Recaption Model (Qwen2VL)</a>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

  <!-- Abstract Section -->
  <section>
    <div class="container">
      <div class="section-header">
        <h2>Abstract</h2>
      </div>
      
      <div class="content">
        <p>Large-scale but noisy image-text pair data have paved the way for the success of Contrastive Language-Image Pretraining (CLIP). As the foundation vision encoder, CLIP in turn serves as the cornerstone for most large vision-language models (LVLMs). This interdependence naturally raises an interesting question: Can we reciprocally leverage LVLMs to enhance the quality of image-text pair data, thereby opening the possibility of a self-reinforcing cycle for continuous improvement? In this work, we take a significant step toward this vision by introducing an LVLM-driven data refinement pipeline.</p>
        
        <p>Our framework leverages LVLMs to process images and their raw alt-text, generating <span class="highlight">four complementary textual formulas</span>: long positive descriptions, long negative descriptions, short positive tags, and short negative tags. Applying this pipeline to the curated DFN-Large dataset yields <span class="highlight">VLM-150M</span>, a refined dataset enriched with multi-grained annotations. Based on this dataset, we further propose a training paradigm that extends conventional contrastive learning by incorporating negative descriptions and short tags as additional supervised signals.</p>
        
        <p>The resulting model, <span class="highlight">HQ-CLIP</span>, demonstrates remarkable improvements across diverse benchmarks. Within a comparable training data scale, our approach achieves state-of-the-art performance in zero-shot classification, cross-modal retrieval, and fine-grained visual understanding tasks. In retrieval benchmarks, HQ-CLIP even surpasses standard CLIP models trained on the DFN-2B dataset, which contains <span class="highlight">10x more training data than ours</span>.</p>
      </div>
    </div>
  </section>

  <!-- Problem and Solution Section -->
  <section class="problem-solution">
    <div class="container">
      <div class="section-header">
        <h2>Research Challenge</h2>
      </div>
      
      <div class="ps-container">
        <div class="ps-item">
          <h3 class="ps-title">Problem</h3>
          <div class="content">
            <p>Existing methods for enhancing CLIP with LLMs suffer from limitations:</p>
            <ul style="padding-left: 1.5rem; margin-top: 1rem;">
              <li>Single-modality approaches (LaCLIP, WhatIf) neglect cross-modal correlations</li>
              <li>Hybrid methods (CapFusion, VeCLIP) introduce computational complexity</li>
              <li>Cascade pipelines cause potential error propagation</li>
              <li>Information asymmetry between visual and textual modalities</li>
              <li>High computational cost of using SoTA LVLMs at scale</li>
            </ul>
          </div>
        </div>
        
        <div class="ps-item">
          <h3 class="ps-title">Our Solution</h3>
          <div class="content">
            <p>HQ-CLIP introduces a unified approach:</p>
            <ul style="padding-left: 1.5rem; margin-top: 1rem;">
              <li>Single LVLM processes both images and paired texts simultaneously</li>
              <li>Generates <span class="highlight">four complementary descriptions</span>: long/short positives and negatives</li>
              <li>Cost-efficient pipeline: SFT on compact LVLMs after GPT-4o curation</li>
              <li>Extension of contrastive learning framework with two innovations:
                <ul style="padding-left: 1.5rem;">
                  <li>Short-Tag Classification (STC) loss</li>
                  <li>Hard Negative Identification (HNI) mechanism</li>
                </ul>
              </li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Approach Section -->
  <section>
    <div class="container">
      <div class="section-header">
        <h2>Approach Overview</h2>
      </div>
      
      <div class="approach-container">
        <div class="approach-text">
          <div class="content">
            <p>Our approach creates a unified data generation pipeline where a single LVLM simultaneously processes both images and paired texts to generate enriched textual descriptions. The framework has two key design considerations:</p>
            
            <h3 style="color: var(--primary); margin: 1.5rem 0 1rem;">Model Selection Strategy</h3>
            <p>To address the scalability challenge of using SoTA LVLMs (GPT-4o, Gemini) for large-scale datasets, we introduce a cost-efficient paradigm:</p>
            <ol style="padding-left: 1.5rem;">
              <li>Curate 10,000 high-quality recaption samples using GPT-4o</li>
              <li>Perform supervised fine-tuning (SFT) on compact open-source LVLMs</li>
              <li>Deploy fine-tuned LVLMs for efficient large-scale data processing</li>
            </ol>
            
            <h3 style="color: var(--primary); margin: 1.5rem 0 1rem;">Multi-Formulation Description</h3>
            <p>Our methodology synthesizes four complementary textual formulations:</p>
          </div>
        </div>
        
        <div class="approach-image">
          <img src="https://via.placeholder.com/600x350/4a7bab/ffffff?text=Framework+Diagram" alt="HQ-CLIP Framework">
          <p class="approach-caption">Figure: LVLM-driven data processing pipeline and HQ-CLIP training paradigm</p>
        </div>
      </div>
      
      <div class="methodology-items">
        <div class="method-item">
          <div class="method-icon">üìú</div>
          <h3 class="method-title">Long Positive Descriptions</h3>
          <div class="content">
            <p>Detailed, contextual descriptions aligned with the image content, providing richer information over raw text data.</p>
          </div>
        </div>
        
        <div class="method-item">
          <div class="method-icon">üö´</div>
          <h3 class="method-title">Long Negative Descriptions</h3>
          <div class="content">
            <p>Contradictory descriptions used in Hard Negative Identification (HNI) to strengthen CLIP's ability to discern subtle discrepancies.</p>
          </div>
        </div>
        
        <div class="method-item">
          <div class="method-icon">üè∑Ô∏è</div>
          <h3 class="method-title">Short Positive Tags</h3>
          <div class="content">
            <p>Concise categorical tags enabling Short-Tag Classification (STC) for discrete classification targets.</p>
          </div>
        </div>
        
        <div class="method-item">
          <div class="method-icon">‚õî</div>
          <h3 class="method-title">Short Negative Tags</h3>
          <div class="content">
            <p>Contrastive tags providing fine-grained discriminative signals for visual-textual alignment.</p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Results Section -->
  <section class="results-section">
    <div class="container">
      <div class="section-header">
        <h2>Experimental Results</h2>
      </div>
      
      <div class="content">
        <p>We evaluated HQ-CLIP on a comprehensive set of 38 benchmark datasets across multiple domains. Our method demonstrates:</p>
        <ul style="padding-left: 1.5rem;">
          <li><span class="highlight">State-of-the-art performance</span> in zero-shot classification with comparable training data</li>
          <li>Exceptional cross-modal retrieval capabilities surpassing models trained on 10x more data</li>
          <li>Superior performance as a vision backbone for LLaVA-1.5</li>
        </ul>
      </div>
      
      <div class="results-grid">
        <div class="results-item">
          <div class="result-image">
            <img src="https://via.placeholder.com/400x200/2c5e92/ffffff?text=Zero-Shot+Classification" alt="Zero-shot Classification Results">
          </div>
          <div class="result-content">
            <h3 class="result-title">Zero-Shot Classification</h3>
            <p>HQ-CLIP outperforms competitive baselines across multiple datasets and data scales.</p>
          </div>
        </div>
        
        <div class="results-item">
          <div class="result-image">
            <img src="https://via.placeholder.com/400x200/4a7bab/ffffff?text=Cross-Modal+Retrieval" alt="Cross-Modal Retrieval Results">
          </div>
          <div class="result-content">
            <h3 class="result-title">Cross-Modal Retrieval</h3>
            <p>Superior retrieval performance despite using only 15% of training data compared to DFN-2B.</p>
          </div>
        </div>
        
        <div class="results-item">
          <div class="result-image">
            <img src="https://via.placeholder.com/400x200/e63946/ffffff?text=LVLM+Visual+Backbone" alt="LVLM Visual Backbone Results">
          </div>
          <div class="result-content">
            <h3 class="result-title">LVLM Visual Backbone</h3>
            <p>When integrated into LLaVA-1.5, HQ-CLIP achieves performance improvements on VQA and captioning tasks.</p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Contributions Section -->
  <section class="contributions">
    <div class="container">
      <div class="section-header">
        <h2>Key Contributions</h2>
      </div>
      
      <div class="content">
        <ul class="contrib-list">
          <li>We introduce an <span class="highlight">efficient and effective LVLM-driven data refinement pipeline</span> and apply it to DFN-Large, creating <span class="highlight">VLM-150M</span>, a high-quality dataset comprising 150 million image-text pairs with multi-grained descriptions generated by state-of-the-art LVLMs.</li>
          
          <li>We propose <span class="highlight">HQ-CLIP</span>, a specialized framework that combines Hard Negative Identification (HNI) for fine-grained understanding and Short-Tag Classification (STC) for categorical semantic recognition.</li>
          
          <li>Through large-scale experiments across three orders of magnitude (1M to 150M samples) and evaluation across 38 benchmark datasets, HQ-CLIP demonstrates <span class="highlight">state-of-the-art zero-shot generalization</span>. The model demonstrates exceptional cross-modal retrieval capabilities, surpassing the DFN-2B.</li>
          
          <li>When deployed as the visual backbone for LLaVA-1.5, HQ-CLIP <span class="highlight">outperforms other ViT-B architectures</span> at comparable pre-training scales, showcasing its potential as a superior vision encoder for LVLMs.</li>
        </ul>
      </div>
    </div>
  </section>

  <!-- BibTeX Section -->
  <section class="bibtex-section">
    <div class="container">
      <div class="section-header">
        <h2>Citation</h2>
      </div>
      
      <div class="bibtex-code">
@inproceedings{wei2025hqclip,
  title     = {{HQ-CLIP}: Leveraging Large Vision-Language Models to Create High-Quality Image-Text Datasets and {CLIP} Models},
  author    = {Wei, Zhixiang and Wang, Guangting and Ma, Xiaoxiao and Mei, Ke and Chen, Huaian and Jin, Yi and Rao, Fengyun},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  year      = {2025}
}</div>
    </div>
  </section>

  <!-- Footer -->
  <footer>
    <div class="container">
      <div class="footer-text">
        <p>This project is part of research conducted at University of Science and Technology of China and WeChat Vision, Tencent Inc. All code, models and datasets will be released upon publication.</p>
        <p class="copyright">¬© 2025 HQ-CLIP Project Team | ICCV 2025</p>
      </div>
    </div>
  </footer>

</body>
</html>