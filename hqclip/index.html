<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>[ICCV 2025] HQ-CLIP</title>

  <!-- Meta Tags for SEO and Social Sharing -->
  <meta name="description" content="[ICCV 2025] HQ-CLIP: Leveraging Large Vision-Language Models to Create High-Quality Image-Text Datasets and CLIP Models">
  <meta name="keywords" content="HQ-CLIP, Vision-Language Models, CLIP, Image-Text Datasets, ICCV 2025">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Open Graph / Facebook -->
  <meta property="og:title" content="HQ-CLIP: Leveraging Large Vision-Language Models to Create High-Quality Image-Text Datasets and CLIP Models"/>
  <meta property="og:description" content="Project page for HQ-CLIP, presented at ICCV 2025. We introduce an LVLM-driven data refinement pipeline to create high-quality datasets and CLIP models."/>
  <meta property="og:url" content="https://zxwei.site/hqclip"/>
  <meta property="og:image" content="https://zxwei.site/images/iccv25.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>
  <meta property="og:type" content="website" />

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="HQ-CLIP: Leveraging Large Vision-Language Models to Create High-Quality Image-Text Datasets and CLIP Models">
  <meta name="twitter:description" content="Project page for HQ-CLIP, presented at ICCV 2025. We introduce an LVLM-driven data refinement pipeline to create high-quality datasets and CLIP models.">
  <meta name="twitter:image" content="https://zxwei.site/images/iccv25.png">

  <!-- Favicon and Fonts -->
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <!-- Stylesheets -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <!-- JavaScript -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>

<!-- Title, Authors, and Links Section -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">HQ-CLIP:</h1>
          <h2 class="subtitle is-2 publication-title">Leveraging Large Vision-Language Models to Create High-Quality Image-Text Datasets and CLIP Models</h2>
          
          <div class="is-size-5 publication-authors">
            <span class="author-block"><a href="https://zxwei.site" target="_blank" rel="noopener noreferrer">Zhixiang Wei</a><sup>1,2,*</sup>,</span>
            <span class="author-block"><a href="https://scholar.google.com/citations?user=cKY8e8sAAAAJ&hl=zh-CN" target="_blank" rel="noopener noreferrer">Guangting Wang</a><sup>2,*</sup>,</span>
            <span class="author-block"><a href="https://krennic999.github.io/" target="_blank" rel="noopener noreferrer">Xiaoxiao Ma</a><sup>1</sup>,</span>
            <span class="author-block">Ke Mei<sup>2</sup>,</span>
            <span class="author-block">Huaian Chen<sup>1</sup>,</span>
            <span class="author-block">Yi Jin<sup>1</sup>,</span>
            <span class="author-block">Fengyun Rao<sup>2</sup></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Science and Technology of China</span>
            <span class="author-block"><sup>2</sup>WeChat Vision, Tencent Inc.</span>
            <br>
            <span class="author-block">ICCV 2025</span>
            <br>
            <span class="eql-cntrb"><small><sup>*</sup>Indicates Equal Contribution</small></span>
          </div>

          <!-- Publication Links -->
          <div class="publication-links">
            <!-- Row 1: Core Links -->
            <div class="button-row">
              <span class="link-block">
                <a href="https://arxiv.org/abs/2312.04265" target="_blank" rel="noopener noreferrer" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="ai ai-arxiv"></i></span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/w1oves/hqclip" target="_blank" rel="noopener noreferrer" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fab fa-github"></i></span>
                  <span>Code</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/spaces/zhixiangwei/hqclip" target="_blank" rel="noopener noreferrer" class="external-link button is-normal is-rounded is-info">
                  <span class="icon"><i class="fas fa-play-circle"></i></span>
                  <span>Live Demo</span>
                </a>
              </span>
            </div>
            <!-- Row 2: Datasets -->
            <div class="button-row">
              <span class="link-block">
                <a href="https://huggingface.co/datasets/zhixiangwei/VLM-150M" target="_blank" rel="noopener noreferrer" class="external-link button is-normal is-rounded is-warning">
                  <span class="icon"><i class="fas fa-database"></i></span>
                  <span>VLM-150M Dataset</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/datasets/zhixiangwei/VLM-1B" target="_blank" rel="noopener noreferrer" class="external-link button is-normal is-rounded is-warning">
                  <span class="icon"><i class="fas fa-database"></i></span>
                  <span>VLM-1B Dataset</span>
                </a>
              </span>
            </div>
            <!-- Row 3: Models -->
            <div class="button-row">
              <span class="link-block">
                <a href="https://huggingface.co/zhixiangwei/qwen2-7b-full" target="_blank" rel="noopener noreferrer" class="external-link button is-normal is-rounded is-success">
                  <span class="icon"><i class="fas fa-brain"></i></span>
                  <span>Recaption Model</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/zhixiangwei/vlm150m-hqclip-large-vitb16" target="_blank" rel="noopener noreferrer" class="external-link button is-normal is-rounded is-success">
                  <span class="icon"><i class="fas fa-cube"></i></span>
                  <span>HQ-CLIP-B</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/zhixiangwei/vlm1b-hqclip-xlarge-vitl14-clipa" target="_blank" rel="noopener noreferrer" class="external-link button is-normal is-rounded is-success">
                  <span class="icon"><i class="fas fa-cube"></i></span>
                  <span>HQ-CLIP-L</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/zhixiangwei/hqclip-openai-large-ft-vlm1b" target="_blank" rel="noopener noreferrer" class="external-link button is-normal is-rounded is-success">
                  <span class="icon"><i class="fas fa-cube"></i></span>
                  <span>HQ-OpenAI-CLIP</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser Section -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full has-text-centered">
        <h2 class="title is-2">Teaser</h2>
        <img src="static/teaser.jpg" alt="Teaser image for HQ-CLIP showing dataset synthesis and performance comparison."/>
        <div class="content has-text-justified" style="margin-top: 1rem;">
          <p>
            (a) We efficiently synthesize <strong>150 million high-quality image-text pairs</strong> using a Large Vision-Language Model (LVLM), with each pair containing four complementary texts: a long positive description, a long negative description, short positive tags, and short negative tags.
            (b) With a comparable scale of training data, our method <strong>achieves state-of-the-art performance</strong> across multiple downstream benchmarks.
            (c) Using the same model architecture, our method's <strong>retrieval performance surpasses models trained on 2 billion data points</strong>, demonstrating superior data efficiency.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Abstract Section -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-2">Abstract</h2>
    <div class="content has-text-justified">
      <p>
        Large-scale but noisy image-text pair data have paved the way for the success of Contrastive Language-Image Pretraining (CLIP). As the foundation vision encoder, CLIP in turn serves as the cornerstone for most large vision-language models (LVLMs). This interdependence naturally raises an interesting question: Can we reciprocally leverage LVLMs to enhance the quality of image-text pair data, thereby opening the possibility of a self-reinforcing cycle for continuous improvement? In this work, we take a significant step toward this vision by introducing an LVLM-driven data refinement pipeline. Our framework leverages LVLMs to process images and their raw alt-text, generating four complementary textual formulas: long positive descriptions, long negative descriptions, short positive tags, and short negative tags. Applying this pipeline to the curated DFN-Large dataset yields <strong>VLM-150M</strong>, a refined dataset enriched with multi-grained annotations. Based on this dataset, we further propose a training paradigm that extends conventional contrastive learning by incorporating negative descriptions and short tags as additional supervised signals. The resulting model, namely <strong>HQ-CLIP</strong>, demonstrates remarkable improvements across diverse benchmarks. Within a comparable training data scale, our approach achieves state-of-the-art performance in zero-shot classification, cross-modal retrieval, and fine-grained visual understanding tasks. In retrieval benchmarks, HQ-CLIP even surpasses standard CLIP models trained on the DFN-2B dataset, which contains 10x more training data than ours. All code, data, and models will be made publicly available to support further research.
      </p>
    </div>
  </div>
</section>

<!-- Framework and Results Section -->
<section class="section">
  <div class="container is-max-desktop">
    <!-- Framework -->
    <div class="columns is-centered">
      <div class="column is-full has-text-centered">
        <h2 class="title is-2">Framework</h2>
        <figure class="image">
          <img src="static/framework.jpg" alt="Diagram of the HQ-CLIP framework, showing the LVLM-driven dataset refinement pipeline and the training strategy."/>
        </figure>
        <p class="content" style="margin-top: 1rem;">The framework of our efficient LVLM-driven dataset refinement pipeline and HQ-CLIP training strategy.</p>
      </div>
    </div>
    
    <!-- Results -->
    <h2 class="title is-2 has-text-centered" style="margin-top: 3rem;">Results</h2>

    <!-- Image 1: Table 2 -->
    <div class="columns is-centered">
      <div class="column is-full has-text-centered">
        <figure class="image">
          <img src="static/table2.png"/>
        </figure>
        <p class="content" style="margin-top: 0.5rem;">Table 2: Zero-shot classification results on various datasets.</p>
      </div>
    </div>

    <!-- Image 2: Table 3 & 4 -->
    <div class="columns is-centered">
      <div class="column is-full has-text-centered">
        <figure class="image">
          <img src="static/table34.png"/>
        </figure>
        <p class="content" style="margin-top: 0.5rem;">Tables 3 & 4: Cross-modal retrieval performance on Flickr30k and MS-COCO.</p>
      </div>
    </div>

    <!-- Image 3: Figure 4 -->
    <div class="columns is-centered">
      <div class="column is-full has-text-centered">
        <figure class="image">
          <img src="static/fig4.png"/>
        </figure>
        <p class="content" style="margin-top: 0.5rem;">Figure 4: Visualization of retrieval results.</p>
      </div>
    </div>

    <!-- Image 4: Table 5 -->
    <div class="columns is-centered">
      <div class="column is-full has-text-centered">
        <figure class="image">
          <img src="static/table5.png"/>
        </figure>
        <p class="content" style="margin-top: 0.5rem;">Table 5: Performance on fine-grained visual understanding tasks.</p>
      </div>
    </div>
    
  </div>
</section>

<!-- BibTeX Section -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{wei2025hqclip,
  author    = {Wei, Zhixiang and Wang, Guangting and Ma, Xiaoxiao and Mei, Ke and Chen, Huaian and Jin, Yi and Rao, Fengyun},
  title     = {{HQ-CLIP}: Leveraging Large Vision-Language Models to Create High-Quality Image-Text Datasets and {CLIP} Models},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  year      = {2025}
}</code></pre>
  </div>
</section>

<!-- Footer -->
<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank" rel="noopener noreferrer">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank" rel="noopener noreferrer">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank" rel="noopener noreferrer">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Custom CSS for Button Layout -->
<style>
  .publication-title {
    font-family: 'Google Sans', sans-serif;
  }
  .publication-links {
    display: flex;
    flex-direction: column;
    align-items: center;
    margin-top: 20px;
    gap: 10px; /* Space between button rows */
  }
  .button-row {
    display: flex;
    justify-content: center;
    flex-wrap: wrap;
    gap: 10px; /* Space between buttons in a row */
    width: 100%;
  }
  .link-block {
    display: inline-block;
  }
  /* Responsive design for smaller screens */
  @media (max-width: 768px) {
    .button-row {
      flex-direction: column;
      align-items: center;
    }
    .link-block {
      width: 100%;
      max-width: 300px; /* Set a max-width for buttons on mobile */
    }
    .link-block .button {
      width: 100%;
    }
  }
</style>

</body>
</html>