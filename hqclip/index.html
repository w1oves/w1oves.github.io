<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>[ICCV 2025] HQ-CLIP</title>

  <!-- Meta Tags for SEO and Social Sharing -->
  <meta name="description" content="[ICCV 2025] HQ-CLIP: Leveraging Large Vision-Language Models to Create High-Quality Image-Text Datasets and CLIP Models">
  <meta name="keywords" content="HQ-CLIP, Vision-Language Models, CLIP, Image-Text Datasets, ICCV 2025">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Open Graph / Facebook -->
  <meta property="og:title" content="HQ-CLIP: Leveraging Large Vision-Language Models to Create High-Quality Image-Text Datasets and CLIP Models"/>
  <meta property="og:description" content="Project page for HQ-CLIP, presented at ICCV 2025. We introduce an LVLM-driven data refinement pipeline to create high-quality datasets and CLIP models."/>
  <meta property="og:url" content="https://zxwei.site/hqclip"/>
  <meta property="og:image" content="https://zxwei.site/images/iccv25.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>
  <meta property="og:type" content="website" />

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="HQ-CLIP: Leveraging Large Vision-Language Models to Create High-Quality Image-Text Datasets and CLIP Models">
  <meta name="twitter:description" content="Project page for HQ-CLIP, presented at ICCV 2025. We introduce an LVLM-driven data refinement pipeline to create high-quality datasets and CLIP models.">
  <meta name="twitter:image" content="https://zxwei.site/images/iccv25.png">

  <!-- Favicon and Fonts -->
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <!-- Stylesheets -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <!-- JavaScript -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  
  <style>
    /* Additional styling for improved readability */
    .publication-title {
      font-family: 'Google Sans', sans-serif;
      line-height: 1.1;
    }
    .content p {
      line-height: 1.6;
      margin-bottom: 1rem;
    }
    .section {
      padding: 3rem 1.5rem;
    }
    .hero-body {
      padding: 3rem 0;
    }
    .title.is-2 {
      margin-top: 2rem;
      margin-bottom: 1rem;
      padding-top: 0;
    }
    .is-size-5 {
      margin: 0.5rem 0;
    }
    .author-block {
      margin: 0.2rem 0;
    }
    .publication-links {
      margin-top: 30px;
    }
    .button-row {
      display: flex;
      justify-content: center;
      flex-wrap: wrap;
      gap: 12px;
      width: 100%;
      margin-bottom: 15px;
    }
    .link-block {
      display: inline-block;
      margin: 5px;
    }
    .button {
      font-weight: 500;
    }
    .hero-subtitle {
      font-size: 1.6rem;
    }
    .results-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(350px, 1fr));
      gap: 20px;
      margin-top: 20px;
    }
    .results-item {
      background: #f9f9f9;
      border-radius: 8px;
      padding: 15px;
      box-shadow: 0 2px 10px rgba(0,0,0,0.05);
    }
    .caption {
      font-size: 0.9rem;
      color: #555;
      text-align: center;
      margin-top: 10px;
    }
    .teaser-image {
      border-radius: 8px;
      box-shadow: 0 5px 15px rgba(0,0,0,0.1);
      max-height: 450px;
      object-fit: contain;
    }
    /* Responsive adjustments */
    @media (max-width: 768px) {
      .button-row {
        flex-direction: column;
        align-items: center;
      }
      .link-block {
        width: 100%;
        max-width: 280px;
      }
      .title {
        font-size: 1.8rem !important;
      }
      .hero-subtitle {
        font-size: 1.4rem;
      }
      .results-grid {
        grid-template-columns: 1fr;
      }
    }
  </style>
</head>
<body>

<!-- Title, Authors, and Links Section -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">HQ-CLIP:</h1>
          <h2 class="subtitle hero-subtitle">Leveraging Large Vision-Language Models to Create High-Quality Image-Text Datasets and CLIP Models</h2>
          
          <div class="is-size-5 publication-authors">
            <span class="author-block"><a href="https://zxwei.site" target="_blank" rel="noopener noreferrer">Zhixiang Wei</a><sup>1,2,*</sup>,</span>
            <span class="author-block"><a href="https://scholar.google.com/citations?user=cKY8e8sAAAAJ&hl=zh-CN" target="_blank" rel="noopener noreferrer">Guangting Wang</a><sup>2,*</sup>,</span>
            <span class="author-block"><a href="https://krennic999.github.io/" target="_blank" rel="noopener noreferrer">Xiaoxiao Ma</a><sup>1</sup>,</span>
            <span class="author-block">Ke Mei<sup>2</sup>,</span>
            <span class="author-block">Huaian Chen<sup>1</sup>,</span>
            <span class="author-block">Yi Jin<sup>1</sup>,</span>
            <span class="author-block">Fengyun Rao<sup>2</sup></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Science and Technology of China</span>
            <span class="author-block"><sup>2</sup>WeChat Vision, Tencent Inc.</span>
            <br>
            <span class="author-block">ICCV 2025</span>
            <br>
            <span class="eql-cntrb"><small><sup>*</sup>Indicates Equal Contribution</small></span>
          </div>

          <!-- Publication Links -->
          <div class="publication-links">
            <!-- Row 1: Core Links -->
            <div class="button-row">
              <span class="link-block">
                <a href="https://arxiv.org/abs/2312.04265" target="_blank" rel="noopener noreferrer" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="ai ai-arxiv"></i></span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/w1oves/hqclip" target="_blank" rel="noopener noreferrer" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fab fa-github"></i></span>
                  <span>Code</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/spaces/zhixiangwei/hqclip" target="_blank" rel="noopener noreferrer" class="external-link button is-normal is-rounded is-info">
                  <span class="icon"><i class="fas fa-play-circle"></i></span>
                  <span>Live Demo</span>
                </a>
              </span>
            </div>
            <!-- Row 2: Datasets -->
            <div class="button-row">
              <span class="link-block">
                <a href="https://huggingface.co/datasets/zhixiangwei/VLM-150M" target="_blank" rel="noopener noreferrer" class="external-link button is-normal is-rounded is-warning">
                  <span class="icon"><i class="fas fa-database"></i></span>
                  <span>VLM-150M Dataset</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/datasets/zhixiangwei/VLM-1B" target="_blank" rel="noopener noreferrer" class="external-link button is-normal is-rounded is-warning">
                  <span class="icon"><i class="fas fa-database"></i></span>
                  <span>VLM-1B Dataset</span>
                </a>
              </span>
            </div>
            <!-- Row 3: Models -->
            <div class="button-row">
              <span class="link-block">
                <a href="https://huggingface.co/zhixiangwei/qwen2-7b-full" target="_blank" rel="noopener noreferrer" class="external-link button is-normal is-rounded is-success">
                  <span class="icon"><i class="fas fa-brain"></i></span>
                  <span>Recaption Model</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/zhixiangwei/vlm150m-hqclip-large-vitb16" target="_blank" rel="noopener noreferrer" class="external-link button is-normal is-rounded is-success">
                  <span class="icon"><i class="fas fa-cube"></i></span>
                  <span>HQ-CLIP-B</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/zhixiangwei/vlm1b-hqclip-xlarge-vitl14-clipa" target="_blank" rel="noopener noreferrer" class="external-link button is-normal is-rounded is-success">
                  <span class="icon"><i class="fas fa-cube"></i></span>
                  <span>HQ-CLIP-L</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/zhixiangwei/hqclip-openai-large-ft-vlm1b" target="_blank" rel="noopener noreferrer" class="external-link button is-normal is-rounded is-success">
                  <span class="icon"><i class="fas fa-cube"></i></span>
                  <span>HQ-OpenAI-CLIP</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser Section -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full has-text-centered">
        <h2 class="title is-2">Teaser</h2>
        <img src="static/teaser.jpg" alt="HQ-CLIP teaser: Dataset synthesis and performance comparison" class="teaser-image"/>
        <div class="content has-text-justified" style="margin-top: 1.5rem;">
          <p>
            (a) We efficiently synthesize <strong>150 million high-quality image-text pairs</strong> using a Large Vision-Language Model (LVLM), with each pair containing four complementary texts: a long positive description, a long negative description, short positive tags, and short negative tags.
            <br><br>
            (b) With comparable training data scale, our method achieves <strong>state-of-the-art performance</strong> across multiple downstream benchmarks.
            <br><br>
            (c) Using the same architecture, our method's <strong>retrieval performance surpasses models trained on 2 billion data points</strong>, demonstrating superior data efficiency.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Abstract Section -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-2">Abstract</h2>
    <div class="content has-text-justified">
      <p>
        Large-scale but noisy image-text pair data have paved the way for the success of Contrastive Language-Image Pretraining (CLIP). As the foundation vision encoder, CLIP in turn serves as the cornerstone for most large vision-language models (LVLMs). This interdependence naturally raises an interesting question: Can we reciprocally leverage LVLMs to enhance the quality of image-text pair data, thereby opening the possibility of a self-reinforcing cycle for continuous improvement? In this work, we take a significant step toward this vision by introducing an LVLM-driven data refinement pipeline.
      </p>
      <p>
        Our framework leverages LVLMs to process images and their raw alt-text, generating four complementary textual formulas: long positive descriptions, long negative descriptions, short positive tags, and short negative tags. Applying this pipeline to the curated DFN-Large dataset yields <strong>VLM-150M</strong>, a refined dataset enriched with multi-grained annotations. Based on this dataset, we further propose a training paradigm that extends conventional contrastive learning by incorporating negative descriptions and short tags as additional supervised signals.
      </p>
      <p>
        The resulting model, namely <strong>HQ-CLIP</strong>, demonstrates remarkable improvements across diverse benchmarks. Within a comparable training data scale, our approach achieves state-of-the-art performance in zero-shot classification, cross-modal retrieval, and fine-grained visual understanding tasks. In retrieval benchmarks, HQ-CLIP even surpasses standard CLIP models trained on the DFN-2B dataset, which contains 10x more training data than ours. All code, data, and models will be made publicly available to support further research.
      </p>
    </div>
  </div>
</section>

<!-- Framework and Results Section -->
<section class="section">
  <div class="container is-max-desktop">
    <!-- Framework -->
    <div class="columns is-centered">
      <div class="column is-full has-text-centered">
        <h2 class="title is-2">Framework</h2>
        <figure class="image">
          <img src="static/framework.jpg" alt="HQ-CLIP framework: Dataset refinement pipeline and training strategy"/>
        </figure>
        <p class="content caption">Our LVLM-driven dataset refinement pipeline and HQ-CLIP training strategy</p>
      </div>
    </div>
    
    <!-- Results -->
    <h2 class="title is-2 has-text-centered" style="margin-top: 3rem;">Experimental Results</h2>

    <div class="results-grid">
      <!-- Zero-Shot Classification -->
      <div class="results-item">
        <figure class="image">
          <img src="static/table2.png" alt="Zero-shot classification results across datasets"/>
        </figure>
        <p class="caption">Table 2: Comparison with state-of-the-art models on zero-shot classification tasks</p>
      </div>
      
      <!-- Ablation Studies -->
      <div class="results-item">
        <figure class="image">
          <img src="static/table34.png" alt="Ablation study results"/>
        </figure>
        <p class="caption">Tables 3 & 4: Ablation studies validating design choices</p>
      </div>
      
      <!-- Retrieval Visualization -->
      <div class="results-item">
        <figure class="image">
          <img src="static/fig4.png" alt="Visualization of retrieval results"/>
        </figure>
        <p class="caption">Figure 4: Qualitative retrieval results showing improved accuracy</p>
      </div>
      
      <!-- LVLM Performance -->
      <div class="results-item">
        <figure class="image">
          <img src="static/table5.png" alt="Performance as LVLM visual encoder"/>
        </figure>
        <p class="caption">Table 5: Performance as the visual encoder for LVLMs on various tasks</p>
      </div>
    </div>
  </div>
</section>

<!-- Paper poster -->
<section class="section">
  <div class="container">
    <h2 class="title is-2 has-text-centered">Conference Poster</h2>
    <div class="columns is-centered">
      <div class="column is-two-thirds">
        <figure class="image">
          <img src="static/posters.jpg" alt="ICCV 2025 Conference Poster" style="border-radius: 8px; box-shadow: 0 5px 15px rgba(0,0,0,0.1);">
          <p class="caption">HQ-CLIP poster presented at ICCV 2025</p>
        </figure>
      </div>
    </div>
  </div>
</section>
<!--End paper poster -->

<!-- BibTeX Section -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title is-2">BibTeX</h2>
    <pre style="background: #f8f9fa; padding: 1.5rem; border-radius: 8px;"><code>@inproceedings{wei2025hqclip,
  author    = {Wei, Zhixiang and Wang, Guangting and Ma, Xiaoxiao and Mei, Ke and Chen, Huaian and Jin, Yi and Rao, Fengyun},
  title     = {{HQ-CLIP}: Leveraging Large Vision-Language Models to Create High-Quality Image-Text Datasets and {CLIP} Models},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  year      = {2025}
}</code></pre>
  </div>
</section>

<!-- Footer -->
<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content has-text-centered">
          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank" rel="noopener noreferrer">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank" rel="noopener noreferrer">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer.
            <br>
            This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank" rel="noopener noreferrer">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>